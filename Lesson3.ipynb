{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: 데이터 패키징\n",
    "\n",
    "Deeplearning.AI & Upstage의 다음 강의를 듣고 정리한 노트북입니다.\n",
    "\n",
    "https://learn.deeplearning.ai/courses/pretraining-llms/lesson/4/packaging-data-for-pretraining\n",
    "\n",
    "이 노트북에서는 LLM 사전 훈련을 위한 데이터를 토크나이징 한 후 적절한 길이로 패킹하는 방법에 대해 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이징\n",
    "\n",
    "이전 레슨에서 저장한 데이터를 불러옵니다. 다만 원할한 학습을 위해 `shard` 메소드를 이용하여 데이터의 1/10 만 사용하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"data/preprocessed_dataset.parquet\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shard(num_shards=10, index=0)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이저를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_path_or_name = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I', \"'\", 'm', '▁a', '▁short', '▁sentence']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I'm a short sentence\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이징된 결과에 bos/eos 토큰을 추가하는 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    tokens = tokenizer.tokenize(example['text'])\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # Add <bos>, <eos> tokens to the front and back of tokens_ids \n",
    "    # bos: begin of sequence, eos: end of sequence\n",
    "    token_ids = [tokenizer.bos_token_id] + token_ids + [tokenizer.eos_token_id]\n",
    "    example['input_ids'] = token_ids\n",
    "    example['num_tokens'] = len(token_ids)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5521aa59987a4a17b43664cbf07ad7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'num_tokens'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenization, load_from_cache_file=False)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text The Colorado Climate Center pr\n",
      "\n",
      "input_ids [1, 415, 15837, 1366, 3314, 6064, 5312, 430, 19102, 304, 1178, 356, 281, 3928, 28725, 9735, 28713, 28725, 264, 1052, 14455, 4623, 28725, 9390, 1452, 274, 28725, 17268, 28713, 28725]\n",
      "\n",
      "num_tokens 549\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[3]\n",
    "\n",
    "print(\"text\", sample[\"text\"][:30]) # \n",
    "print(\"\\ninput_ids\", sample[\"input_ids\"][:30])\n",
    "print(\"\\nnum_tokens\", sample[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 토큰의 갯수를 조회합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4887707"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sum(dataset['num_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 패킹\n",
    "\n",
    "![img](./images/data_packing.png)\n",
    "\n",
    "데이터를 모두 이어붙인 후 적절한 길이로 자르는 패킹을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4887707\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.concatenate(dataset['input_ids'])\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4887680\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 64\n",
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "print(total_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 길이로 떨어지지 않는 나머지 부분은 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4887680,)\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids[:total_length]\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76370, 64)\n"
     ]
    }
   ],
   "source": [
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "print(input_ids_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_ids_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 76370\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict(\n",
    "    {'input_ids':input_ids_list}\n",
    ")\n",
    "\n",
    "print(packaged_pretrain_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23df9e71b08b4183ab32be52d1640654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/77 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19856200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packaged_pretrain_dataset.to_parquet('data/packaged_pretrain_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 250280\n",
      "drwxr-xr-x  4 emart  staff   128B Jul 22 18:06 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  9 emart  staff   288B Jul 22 17:52 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--  1 emart  staff    11M Jul 22 18:06 packaged_pretrain_dataset.parquet\n",
      "-rw-r--r--  1 emart  staff   112M Jul 22 17:51 preprocessed_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -alh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
