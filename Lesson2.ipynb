{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: 데이터 준비\n",
    "\n",
    "Deeplearning.AI & Upstage의 다음 강의를 듣고 정리한 노트북입니다.\n",
    "\n",
    "https://learn.deeplearning.ai/courses/pretraining-llms/lesson/3/data-preparation\n",
    "\n",
    "이 노트북에서는 LLM 사전 훈련을 위한 데이터를 어떻게 준비하는지 학습합니다. Upstage에서는 데이터 준비를 위한 [Dataverse](https://github.com/UpstageAI/dataverse)라는 라이브러리를 개발하여 배포하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 학습을 위한 데이터셋 확보\n",
    "\n",
    "여기서는 데이터를 다음 두 가지 방법으로 확보합니다.\n",
    "\n",
    "1. HuggingFace 데이터셋에서 다운로드\n",
    "2. Github에서 파이썬 스크립트 스크레이핑\n",
    "\n",
    "이후 HuggingFace의 [Dataset](https://huggingface.co/docs/datasets/en/index) 라이브러리를 이용하여 데이터셋을 처리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace에서 데이터 다운로드\n",
    "\n",
    "여기서는 [Red Pajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) 데이터셋의 일부를 가져와서 사용합니다. 원본 데이터셋은 1조 개의 토큰으로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2447df31b3a645e79bbd66a62392bf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ef935dfe0643ff87665710adb8e861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "pretraining_dataset = datasets.load_dataset(\n",
    "    \"upstage/Pretraining_Dataset\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta'],\n",
      "    num_rows: 60000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(pretraining_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 `text` 컬럼만을 사용합니다. 일부 데이터를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home World CEO of Crew Clothing CEO Resigns\n",
      "CEO of Crew Clothing CEO Resigns\n",
      "By Karen Roe [CC BY 2.0], via Wikimedia Commons\n",
      "Crew, a British lifestyle clothing brand, has been sold by Livingbridge, its founder and private equity firm to Exquisite Apparel.\n",
      "However, Crew will be advancing under a new image, as the chief executive who was brought in by Livingbridge in order to develop the brand, Louise Barnes, has resigned following the sale. Barnes attempted to lead a management buyout. However, i\n"
     ]
    }
   ],
   "source": [
    "pretraining_dataset = pretraining_dataset.select_columns(['text'])\n",
    "print(pretraining_dataset[10][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파인튜닝 데이터셋 다운로드\n",
    "\n",
    "Alpaca 모델의 [instruction tuning dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html)을 살펴봅니다. 파인튜닝 데이터셋은 위와는 달리 instruction/input/output 형태로 이루어져 있습니다.\n",
    "\n",
    "우리는 여기서는 파인튜닝 데이터셋은 사용하지 않을 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c220f683b84e4890a6e802435fc86ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6270f86784294f31b6ba54c7f387fafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a8e7528c9149f48a31104eb7ace2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "instruction_dataset = datasets.load_dataset(\n",
    "    \"c-s-ale/alpaca-gpt4-data\",\n",
    "    split=\"train\"\n",
    ")\n",
    "print(instruction_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Determine the most common word in the text.\n",
      "Input: Humans are created in the image of God, from a spiritual perspective and from a physical perspective.\n",
      "Output: The most common word in the text is \"from\" as it appears twice in the sentence.\n"
     ]
    }
   ],
   "source": [
    "i=5535\n",
    "print(\"Instruction: \" + instruction_dataset[i][\"instruction\"] \n",
    "      + \"\\nInput: \" + instruction_dataset[i][\"input\"] \n",
    "      + \"\\nOutput: \" + instruction_dataset[i][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github에서 파이썬 코드 스크레이핑\n",
    "\n",
    "Github에서 파이썬 스크립트를 다운로드해서 `Dataset` 오브젝트로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some required packages\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Path to directory to store python scripts\n",
    "code_dir = \"./code\"\n",
    "\n",
    "if not os.path.exists(code_dir):\n",
    "    os.mkdir(code_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\",\n",
    "    \"https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\",\n",
    "    \"https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\",\n",
    "    \"https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\",\n",
    "    \"https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\",\n",
    "    \"https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\",\n",
    "    \"https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\",\n",
    "    \"https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\",\n",
    "    \"https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on url: https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\n",
      "Working on url: https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\n",
      "Working on url: https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\n",
      "Working on url: https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\n",
      "Working on url: https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\n",
      "Working on url: https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\n",
      "Working on url: https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\n",
      "Working on url: https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\n",
      "Working on url: https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    print(f\"Working on url: {url}\")\n",
    "    response = requests.get(url)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_path = os.path.join(code_dir, file_name)\n",
    "    \n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_subgraph_rewriter.py\n",
      "numpy_mlp.py\n",
      "values.py\n",
      "version.py\n",
      "double_linear_search_recursion.py\n",
      "__init__.py\n",
      "visualize.py\n",
      "module_util.py\n",
      "distribute_coordinator_context.py\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(code_dir)\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = []\n",
    "for f in os.listdir(code_dir):\n",
    "    code_dataset.append(\n",
    "        {'text':open(os.path.join(code_dir, f), 'r').read()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 9\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dataset = datasets.Dataset.from_list(code_dataset)\n",
    "print(code_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 학습 데이터셋과 파이썬 코드 데이터셋을 결합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 60009\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.concatenate_datasets([pretraining_dataset, code_dataset])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정제\n",
    "\n",
    "여기서는 다음 과정을 통해 데이터를 정제합니다.\n",
    "\n",
    "1. 너무 짧은 샘플 삭제\n",
    "2. 한 텍스트 내에 반복문이 많은 경우 삭제\n",
    "3. 중복 데이터 삭제\n",
    "4. 영어가 아닌 데이터 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 너무 짧은 샘플 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def paragraph_length_filter(x):\n",
    "    \"\"\"Return False iff a page has too few lines or lines are too short.\"\"\"\n",
    "    lines = x['text'].split('\\n')\n",
    "    if (\n",
    "        len(lines) < 3\n",
    "        or min(heapq.nlargest(3, [len(line) for line in lines])) < 3\n",
    "    ):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f325c2e99a94f7799b4f52c630480a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/60009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_length_filter,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52357"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한 텍스트 내에 반복문이 많은 경우 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(paragraphs):\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elts = 0\n",
    "    for elt in paragraphs:\n",
    "        if elt in unique_x:\n",
    "            duplicate_elts += 1\n",
    "            duplicate_chars += len(elt)\n",
    "        else:\n",
    "            unique_x.add(elt)\n",
    "    return duplicate_elts, duplicate_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def paragraph_repetition_filter(x):\n",
    "    text = x['text']\n",
    "    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
    "    if paragraphs_duplicates / len(paragraphs) > 0.3:\n",
    "        return False\n",
    "    if char_duplicates / len(text) > 0.2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 중복이 발생한 텍스트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 697), 15, 2845)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset[1891]['text']\n",
    "paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())\n",
    "find_duplicates(paragraphs), len(paragraphs), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q: Adding lines and spaces with \\\\addtocontents{toc} without \\\\addtocontents{ptc} I have a follow-up question to this one:\\nWant \\\\addtocontents{toc} without \\\\addtocontents{ptc}\\nI use the titletoc package and want to add vertical spaces and a line in the table of contents, but NOT in the partial TOC. However, the lines and spaces do appear in all partial TOCs as marked in red in the picture below. The solution in the linked question did not work for me, because I want to add an object and not a section. \\nDoes anybody know how to circumvent this?\\nHere is my MWE:\\n\\\\documentclass{article}\\n\\\\usepackage{titletoc}',\n",
       " '\\\\begin{document}\\n\\\\tableofcontents',\n",
       " '\\\\section{Section1}\\nHere the text of the document begins with Section 1.',\n",
       " '\\\\section{Section2}\\n\\\\startcontents % Want partial TOC for Section2\\n\\\\printcontents{}{1}{}\\nHere is the text of Section 2.\\n\\\\subsection{Subsection2.1}\\nHere is the text of the first Subsection.\\n\\\\subsection{Subsection2.2}\\nHere is the text of the second Subsection.\\n\\\\stopcontents %Stop the contents for partial TOC',\n",
       " '% For some reason I want a line and spaces ONLY in the MAIN TOC, not in the partial TOC\\n\\\\addtocontents{toc}{\\\\protect\\\\addvspace{10pt} \\\\protect{\\\\hrule height 1.2pt} \\\\protect\\\\addvspace{10pt}}',\n",
       " '\\\\section{Section3}\\n\\\\startcontents % Want partial TOC for Section3\\n\\\\printcontents{}{1}{}\\nHere is the text of Section 3.\\n\\\\subsection{Subsection3.1}\\nHere is the text of the first Subsection.\\n\\\\subsection{Subsection3.2}\\nHere is the text of the second Subsection.\\n\\\\stopcontents %Stop the contents for partial TOC',\n",
       " '\\\\end{document}',\n",
       " 'A: Here is a solution. The idea is to add \\\\addtocontents{toc}{\\\\protect\\\\myruleandspace} were \\\\myruleandspace is defined with\\n\\\\newcommand{\\\\myruleandspace}{\\\\addvspace{10pt} \\\\hrule height 1.2pt \\\\addvspace{10pt}}\\n\\\\tableofcontents\\n\\\\renewcommand{\\\\myruleandspace}{}',\n",
       " 'MWE\\n\\\\documentclass{article}\\n\\\\usepackage{titletoc}',\n",
       " '\\\\begin{document}\\n\\\\newcommand{\\\\myruleandspace}{\\\\addvspace{10pt} \\\\hrule height 1.2pt \\\\addvspace{10pt}}\\n\\\\tableofcontents\\n\\\\renewcommand{\\\\myruleandspace}{}',\n",
       " '\\\\section{Section1}\\nHere the text of the document begins with Section 1.',\n",
       " '\\\\section{Section2}\\n\\\\startcontents % Want partial TOC for Section2\\n\\\\printcontents{}{1}{}\\nHere is the text of Section 2.\\n\\\\subsection{Subsection2.1}\\nHere is the text of the first Subsection.\\n\\\\subsection{Subsection2.2}\\nHere is the text of the second Subsection.\\n\\\\stopcontents %Stop the contents for partial TOC',\n",
       " '% For some reason I want a line and spaces ONLY in the MAIN TOC, not in the partial TOC\\n\\\\addtocontents{toc}{\\\\protect\\\\myruleandspace}',\n",
       " '\\\\section{Section3}\\n\\\\startcontents % Want partial TOC for Section3\\n\\\\printcontents{}{1}{}\\nHere is the text of Section 3.\\n\\\\subsection{Subsection3.1}\\nHere is the text of the first Subsection.\\n\\\\subsection{Subsection3.2}\\nHere is the text of the second Subsection.\\n\\\\stopcontents %Stop the contents for partial TOC',\n",
       " '\\\\end{document}']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a134918916574c25bf90655745bbea00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52357 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52327"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_repetition_filter,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 데이터 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be5cf79ad124dafbfecb5e32817884c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "43598"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_text = set()\n",
    "\n",
    "def dedup_func(x):\n",
    "    if x['text'] in unique_text:\n",
    "        return False\n",
    "    else:\n",
    "        unique_text.add(x['text'])\n",
    "        return True\n",
    "    \n",
    "dataset = dataset.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어가 아닌 데이터 삭제\n",
    "\n",
    "특정 언어의 데이터를 추출하기 위해 [FastText](https://fasttext.cc) 라이브러리를 사용합니다. 실제 모델의 웨이트 파일은 [여기](https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin)에서 다운로드 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49934b155574d6bac1c7752370e549d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/43598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40478"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fasttext.FastText import _FastText\n",
    "\n",
    "def english_language_filter(ds):\n",
    "    model = _FastText(\"./models/lid.176.bin\")\n",
    "\n",
    "    def is_english(x):\n",
    "        language, score = model.predict(x['text'].replace('\\n', ' '))\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        return score > 0.4 and language == \"en\" # en을 다른 언어로 변경 가능\n",
    "    \n",
    "    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = english_language_filter(dataset)\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 저장\n",
    "\n",
    "parquet 형식에 대해서는 [여기](https://parquet.apache.org)를 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074f6da03f8a4cc58e5b9d43c234c484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "197041742"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/preprocessed_dataset.parquet\"\n",
    "dataset.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 228720\n",
      "drwxr-xr-x  3 emart  staff    96B Jul 22 17:51 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  8 emart  staff   256B Jul 22 17:51 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--  1 emart  staff   112M Jul 22 17:51 preprocessed_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -alh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
